{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c864d4a-d613-4db5-ab29-8ef631e9ae29",
   "metadata": {
    "azdata_cell_guid": "d9387a7d-db30-4e91-bfd1-f7e383241abc",
    "tags": []
   },
   "source": [
    "# Get Started with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec420f-4867-46b0-834e-8fa8308384f9",
   "metadata": {
    "azdata_cell_guid": "3fb89bca-0596-4cb7-b71a-4de4df95e660"
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ef21c8-a0b7-473e-b79b-2300a45c573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56443575-830c-4da6-9570-0873a106c1dc",
   "metadata": {
    "azdata_cell_guid": "d9fc2a16-2294-4d47-b7bc-40916a6c33c1"
   },
   "source": [
    "## Paths & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45d06ea-3e9e-414f-9ab7-a325692d458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = 'data/iris.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc839f6-cb61-4293-b7d9-631d8fd4bc42",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595f8de3-c509-4162-98de-1d25e4d9ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d06592-4489-45b1-b67f-ad1f3955663f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d906191-d2a3-4cba-a91e-c133f1d93e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read\\\n",
    "                .option('header', True)\\\n",
    "                .csv(DATA, sep=',')\n",
    "spark_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954582b6-19ac-4a05-ae76-bd3e482af5b6",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b99696ed-7715-4929-b29d-422d3732466b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sepal_length', 'string'),\n",
       " ('sepal_width', 'string'),\n",
       " ('petal_length', 'string'),\n",
       " ('petal_width', 'string'),\n",
       " ('species', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe41a64-452d-4a53-8af2-954a613a7bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: string (nullable = true)\n",
      " |-- sepal_width: string (nullable = true)\n",
      " |-- petal_length: string (nullable = true)\n",
      " |-- petal_width: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf94fda-ead4-47ad-9a80-1986cb10dace",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f130093a-fcdb-41ad-8524-81fbee4c0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = spark_df.randomSplit(weights=(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a1505ce-8467-404a-af0f-f44c6a66a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|   species|\n",
      "+------------+-----------+------------+-----------+----------+\n",
      "|         4.3|        3.0|         1.1|        0.1|    setosa|\n",
      "|         4.4|        3.0|         1.3|        0.2|    setosa|\n",
      "|         4.5|        2.3|         1.3|        0.3|    setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|    setosa|\n",
      "|         4.6|        3.2|         1.4|        0.2|    setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3|    setosa|\n",
      "|         4.6|        3.6|         1.0|        0.2|    setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|    setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1|    setosa|\n",
      "|         4.8|        3.0|         1.4|        0.3|    setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2|    setosa|\n",
      "|         4.8|        3.4|         1.9|        0.2|    setosa|\n",
      "|         4.9|        2.4|         3.3|        1.0|versicolor|\n",
      "|         4.9|        2.5|         4.5|        1.7| virginica|\n",
      "|         4.9|        3.1|         1.5|        0.2|    setosa|\n",
      "|         4.9|        3.6|         1.4|        0.1|    setosa|\n",
      "|         5.0|        2.3|         3.3|        1.0|versicolor|\n",
      "|         5.0|        3.0|         1.6|        0.2|    setosa|\n",
      "|         5.0|        3.2|         1.2|        0.2|    setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2|    setosa|\n",
      "+------------+-----------+------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de4f8f-2047-4ab5-a59a-c3f2f53dafab",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d55d2-8b23-4662-a4fa-fb0d9ca5e9da",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d44450-3d61-4cb5-b64f-4cc1a6fd5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b8a915-2b7b-480d-9cf2-664b0ab6b51a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column sepal_length must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.StringType$:string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11668/1058576546.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_scaled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mscaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mscaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mscaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Column sepal_length must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.StringType$:string."
     ]
    }
   ],
   "source": [
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "for feature in features:\n",
    "    scaler = StandardScaler(inputCol=feature, outputCol=feature + \"_scaled\") \n",
    "    scaled = scaler.fit(spark_df).transform(spark_df)\n",
    "    scaled = scaled.drop(feature)\n",
    "scaled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b96686d-111d-45a9-b531-8b89679da537",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"inputCol\". Could not convert <class 'tuple'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36m_set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36mtoString\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not convert %s to string type\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not convert <class 'tuple'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15992/3406081162.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m scaler = StandardScaler(inputCol=(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"), \n\u001b[0m\u001b[0;32m      2\u001b[0m                         outputCol=(\"sepal_length_scaled\", \"sepal_width_scaled\", \"petal_length_scaled\", \"petal_width_scaled\")) \n\u001b[0;32m      3\u001b[0m \u001b[0mscaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sepal_length\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sepal_width\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"petal_length\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"petal_width\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\feature.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, withMean, withStd, inputCol, outputCol)\u001b[0m\n\u001b[0;32m   3505\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"org.apache.spark.ml.feature.StandardScaler\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3506\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3507\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mkeyword_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\feature.py\u001b[0m in \u001b[0;36msetParams\u001b[1;34m(self, withMean, withStd, inputCol, outputCol)\u001b[0m\n\u001b[0;32m   3515\u001b[0m         \"\"\"\n\u001b[0;32m   3516\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3517\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3519\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1.4.0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36m_set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    464\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid param value given for param \"%s\". %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid param value given for param \"inputCol\". Could not convert <class 'tuple'> to string type"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"), \n",
    "                        outputCol=(\"sepal_length_scaled\", \"sepal_width_scaled\", \"petal_length_scaled\", \"petal_width_scaled\")) \n",
    "scaled = scaler.fit(spark_df).transform(spark_df)\n",
    "scaled = scaled.drop(*(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"))\n",
    "scaled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27347ac0-fe3b-4d15-9b2f-b36da6b04075",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc48a1d6-4374-4d89-b759-37d08c52df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f01d3e8-ec02-4316-abeb-93259ce0434f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|speciesIndex|\n",
      "+------------+-----------+------------+-----------+------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|         0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|         0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2|         0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2|         0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2|         0.0|\n",
      "|         5.4|        3.9|         1.7|        0.4|         0.0|\n",
      "|         4.6|        3.4|         1.4|        0.3|         0.0|\n",
      "|         5.0|        3.4|         1.5|        0.2|         0.0|\n",
      "|         4.4|        2.9|         1.4|        0.2|         0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|         0.0|\n",
      "|         5.4|        3.7|         1.5|        0.2|         0.0|\n",
      "|         4.8|        3.4|         1.6|        0.2|         0.0|\n",
      "|         4.8|        3.0|         1.4|        0.1|         0.0|\n",
      "|         4.3|        3.0|         1.1|        0.1|         0.0|\n",
      "|         5.8|        4.0|         1.2|        0.2|         0.0|\n",
      "|         5.7|        4.4|         1.5|        0.4|         0.0|\n",
      "|         5.4|        3.9|         1.3|        0.4|         0.0|\n",
      "|         5.1|        3.5|         1.4|        0.3|         0.0|\n",
      "|         5.7|        3.8|         1.7|        0.3|         0.0|\n",
      "|         5.1|        3.8|         1.5|        0.3|         0.0|\n",
      "+------------+-----------+------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = StringIndexer(inputCol=\"species\", outputCol=\"speciesIndex\") \n",
    "indexed = encoder.fit(spark_df).transform(spark_df)\n",
    "indexed = indexed.drop(\"species\")\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff4da2b-1d56-4b09-a66e-5e6acb959f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list  of students  data \n",
    "data =[[\"1\",\"sravan\",\"vignan\"],\n",
    "       [\"2\",\"ojaswi\",\"vvit\"],\n",
    "       [\"3\",\"rohith\",\"vvit\"],\n",
    "       [\"4\",\"sridevi\",\"vignan\"],\n",
    "       [\"1\",\"sravan\",\"vignan\"], \n",
    "       [\"5\",\"gnanesh\",\"iit\"]]\n",
    "  \n",
    "# specify column names\n",
    "columns=['student ID','student NAME','college']\n",
    "  \n",
    "# creating a dataframe from the lists of data\n",
    "dataframe = spark.createDataFrame(data,columns)\n",
    "  \n",
    "print(\"Actual data in dataframe\")\n",
    "  \n",
    "# show dataframe\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488381ac-d5fe-4abc-aea8-cc7ce78198af",
   "metadata": {
    "azdata_cell_guid": "61721ac4-19b2-423c-9cc2-39fc7e1e9e43"
   },
   "source": [
    "## References\n",
    "* [Install PySpark in Anaconda & Jupyter Notebook](https://sparkbyexamples.com/pyspark/install-pyspark-in-anaconda-jupyter-notebook/)\n",
    "* https://www.geeksforgeeks.org/how-to-delete-columns-in-pyspark-dataframe/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
